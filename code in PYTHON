import numpy as np
input_size=3
layers=[4,3]
output_size=2
def softmax(a):
  e_pa=np.exp(a)
  ans=e_pa/np.sum(e_pa,axis=1,keepdims=True)
  return ans
class NeuralNetwork:
  def __init__(self,input_size,layers,output_size):
    np.random.seed(0)
    model={}
    model['w1']=np.random.randn(input_size,layers[0])
    model['b1']=np.zeros((1,layers[0]))
    model['w2']=np.random.randn(layers[0],layers[1])
    model['b2']=np.zeros((1,layers[1]))
    model['w3']=np.random.randn(layers[1],output_size)
    model['b3']=np.zeros((1,output_size))
    self.model=model
  def forward(self,x):
    w1,w2,w3=self.model['w1'],self.model['w2'],self.model['w3']
    b1,b2,b3=self.model['b1'],self.model['b2'],self.model['b3']
    z1=np.dot(x,w1)+b1
    a1=np.tanh(z1)
    z2=np.dot(a1,w2)+b2
    a2=np.tanh(z2)
    z3=np.dot(a2,w3)+b3
    y_=softmax(z3)
    self.activation_outputs=(a1,a2,y_)
    return y_
  def backward(self,x,y,learning_rate=0.001):
    w1,w2,w3=self.model['w1'],self.model['w2'],self.model['w3']
    b1,b2,b3=self.model['b1'],self.model['b2'],self.model['b3']
    m=x.shape[0]
    a1,a2,y_=self.activation_outputs
    delta3=y_ -y
    dw3=np.dot(a2.T,delta3)
    db3=np.sum(delta3,axis=0)/float(m)

    delta2=(1-np.square(a2))*np.dot(delta3,w3.T)
    dw2=np.dot(a1.T,delta2)
    db2=np.sum(delta2,axis=0)/float(m)

    delta1=(1-np.square(a1))*np.dot(delta2,w2.T)
    dw1=np.dot(x.T,delta1)
    db1=np.sum(delta1,axis=0)/float(m)

    self.model['w1']-=learning_rate*dw1
    self.model['b1']-=learning_rate*db1
    self.model['w2']-=learning_rate*dw2
    self.model['b2']-=learning_rate*db2
    self.model['w3']-=learning_rate*dw3
    self.model['b3']-=learning_rate*db3
  def predict(self,x):
    y_out=self.forward(x)
    return np.argmax(y_out,axis=1)



#LOSS FUCNTION

def loss(y_oht,p):
  q=-np.mean(y_oht*np.log(p))
  return q
#one hot vector 
def one_hot(y,depth):
  m=y.shape[0]
  y_oht=np.zeros((m,depth))
  y_oht[np.arange(m),y]=1
  return y_oht 






    



